{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cnn.com/2020/02/19/business/mcdona...</td>\n",
       "      <td>A new use for McDonald's used cooking oil: 3D ...</td>\n",
       "      <td>Simpson is director of the school's Environmen...</td>\n",
       "      <td>2020-04-16 07:29:14.124708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.cnn.com/2020/04/09/tech/instacart-...</td>\n",
       "      <td>People are luring Instacart shoppers with big ...</td>\n",
       "      <td>But an hour later, Arambula checked her earnin...</td>\n",
       "      <td>2020-04-16 07:29:14.124708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cnn.com/2020/03/16/tech/xbox-serie...</td>\n",
       "      <td>Microsoft reveals details about its next-gener...</td>\n",
       "      <td>The Series X will be capable of running 4K gra...</td>\n",
       "      <td>2020-04-16 07:29:14.124708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cnn.com/2020/04/15/business/oil-pr...</td>\n",
       "      <td>US faces an 'unprecedented' decline in oil pro...</td>\n",
       "      <td>US crude futures dropped as low as $19.20 a ba...</td>\n",
       "      <td>2020-04-16 07:29:14.124708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.cnn.com/2020/04/08/business/restau...</td>\n",
       "      <td>They used to sell food to top chefs. Now you'r...</td>\n",
       "      <td>Fruit and vegetable wholesalers that sold fres...</td>\n",
       "      <td>2020-04-16 07:29:14.124708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.cnn.com/2020/02/19/business/mcdona...   \n",
       "1  https://www.cnn.com/2020/04/09/tech/instacart-...   \n",
       "2  https://www.cnn.com/2020/03/16/tech/xbox-serie...   \n",
       "3  https://www.cnn.com/2020/04/15/business/oil-pr...   \n",
       "4  https://www.cnn.com/2020/04/08/business/restau...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A new use for McDonald's used cooking oil: 3D ...   \n",
       "1  People are luring Instacart shoppers with big ...   \n",
       "2  Microsoft reveals details about its next-gener...   \n",
       "3  US faces an 'unprecedented' decline in oil pro...   \n",
       "4  They used to sell food to top chefs. Now you'r...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Simpson is director of the school's Environmen...   \n",
       "1  But an hour later, Arambula checked her earnin...   \n",
       "2  The Series X will be capable of running 4K gra...   \n",
       "3  US crude futures dropped as low as $19.20 a ba...   \n",
       "4  Fruit and vegetable wholesalers that sold fres...   \n",
       "\n",
       "                        date  \n",
       "0 2020-04-16 07:29:14.124708  \n",
       "1 2020-04-16 07:29:14.124708  \n",
       "2 2020-04-16 07:29:14.124708  \n",
       "3 2020-04-16 07:29:14.124708  \n",
       "4 2020-04-16 07:29:14.124708  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import requests\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "# Function to get proxies so we can rotate through them\n",
    "def scrape_proxies():\n",
    "    proxies = []\n",
    "    headers = {'User-Agent': UserAgent().random}\n",
    "    resp = requests.get('https://www.sslproxies.org/', headers=headers)\n",
    "    site = resp.content\n",
    "    soup = BeautifulSoup(site, 'html.parser')\n",
    "    proxies_table = soup.find(id='proxylisttable')\n",
    "    for row in proxies_table.tbody.find_all('tr'):\n",
    "        proxies.append(row.find_all('td')[0].string + ':' + row.find_all('td')[1].string)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "# Function to scrape the html into beautiful soup\n",
    "def scrape_html(url, max_n_tries, proxies):\n",
    "    num_tries = 0\n",
    "    max_tries = max_n_tries\n",
    "    while num_tries < max_tries: \n",
    "        current_proxy = {\"http\": \"http://{}\".format(random.choice(proxies))}\n",
    "        headers = {'User-Agent': UserAgent().random}\n",
    "        resp = requests.get(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            proxies=current_proxy\n",
    "        )\n",
    "        if resp.status_code == 200:\n",
    "            site = resp.content\n",
    "            soup = BeautifulSoup(site, 'html.parser')\n",
    "            return soup\n",
    "        else:\n",
    "            num_tries += 1\n",
    "            time.sleep(random.randint(1, 2))    \n",
    "\n",
    "\n",
    "# Get all the href tags from the business homepage and add them to a list\n",
    "resp = requests.get('https://www.cnn.com/business')\n",
    "http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "encoding = html_encoding or http_encoding\n",
    "soup = BeautifulSoup(resp.content, from_encoding=encoding)\n",
    "\n",
    "links = [link['href'] for link in soup.find_all('a', href=True)]\n",
    "\n",
    "# Not all hrefs are actual article links\n",
    "# Article links always start with the date\n",
    "# so we'll do a regex for that\n",
    "article_links = []\n",
    "\n",
    "for link in links:\n",
    "    regexp = re.compile(r'(\\d+/\\d+/\\d+)')\n",
    "    if regexp.search(link):\n",
    "        article_links.append('https://www.cnn.com' + link)\n",
    "        \n",
    "# There are duplicate links so we'll remove those\n",
    "article_links = list(set(article_links))\n",
    "\n",
    "# There are also video links so we'll remove those too\n",
    "article_links = [each for each in article_links if '/videos/' not in each]\n",
    "\n",
    "# Scrape each link and add to lists\n",
    "links, titles, body = [], [], []\n",
    "\n",
    "for url in article_links:\n",
    "    links.append(url)\n",
    "    \n",
    "    proxies = scrape_proxies()\n",
    "    soup = scrape_html(url, 5, proxies)\n",
    "    \n",
    "    try:\n",
    "        titles.append(soup.h1.text)\n",
    "    except AttributeError:\n",
    "        titles.append(np.nan)\n",
    "        \n",
    "    p = soup.findAll('div', {'class': 'zn-body__paragraph'})    \n",
    "    p_text = []\n",
    "\n",
    "    for each in p:\n",
    "        try:\n",
    "            p_text.append(each.text)\n",
    "        except AttributeError:\n",
    "            p_text.append(np.nan)\n",
    "            \n",
    "    body.append(' '.join(p_text))\n",
    "    \n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "# Create a df from the lists\n",
    "df = pd.DataFrame({\n",
    "    'url': links,\n",
    "    'title': titles,\n",
    "    'body': body\n",
    "})\n",
    "\n",
    "# Add in today's date\n",
    "df['date'] = pd.to_datetime(datetime.today())\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
